# backend/api/admin.py
"""Í¥ÄÎ¶¨Ïûê API"""
from datetime import date
from backend.common.date_utils import get_today_kst
import json
import os
from typing import Optional
from fastapi import APIRouter, HTTPException, Request, Depends, Query

from backend.schemas.admin import (
    SystemStatus, SchedulerStatus, DatabaseTable, TodayProblemsStatus,
    GenerateProblemsRequest, GenerateProblemsResponse,
    RefreshDataRequest, RefreshDataResponse, ScheduleRunResponse,
    TriggerRCARequest
)
from backend.services.database import postgres_connection, duckdb_connection
from backend.api.auth import get_session
from backend.services.db_logger import get_logs, db_log, LogCategory, LogLevel


async def require_admin(request: Request):
    """Í¥ÄÎ¶¨Ïûê Í∂åÌïú Ï≤¥ÌÅ¨ (ÏÉÅÏÑ∏ Î°úÍπÖ Ìè¨Ìï®)"""
    from backend.common.logging import get_logger
    logger = get_logger(__name__)
    
    session_id = request.cookies.get("session_id")
    if not session_id:
        logger.warning("Admin access denied: No session cookie")
        raise HTTPException(403, "Î°úÍ∑∏Ïù∏Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§ (ÏÑ∏ÏÖò ÏóÜÏùå)")
    
    session = get_session(session_id)
    if not session:
        logger.warning(f"Admin access denied: Session not found (id={session_id[:8]}...)")
        raise HTTPException(403, "ÏÑ∏ÏÖòÏù¥ ÎßåÎ£åÎêòÏóàÏäµÎãàÎã§. Îã§Ïãú Î°úÍ∑∏Ïù∏Ìï¥Ï£ºÏÑ∏Ïöî")
    
    user_email = session.get("user", {}).get("email", "")
    if not user_email:
        logger.warning("Admin access denied: No email in session")
        raise HTTPException(403, "ÏÑ∏ÏÖò Ï†ïÎ≥¥Í∞Ä Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏäµÎãàÎã§")
    
    # DBÏóêÏÑú is_admin ÌôïÏù∏
    try:
        with postgres_connection() as pg:
            df = pg.fetch_df("SELECT is_admin FROM public.users WHERE email = %s", [user_email])
            if len(df) == 0:
                logger.warning(f"Admin access denied: User not found ({user_email})")
                raise HTTPException(403, "ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
            if not df.iloc[0].get('is_admin', False):
                logger.warning(f"Admin access denied: Not admin ({user_email})")
                raise HTTPException(403, "Í¥ÄÎ¶¨Ïûê Í∂åÌïúÏù¥ ÏóÜÏäµÎãàÎã§")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Admin check DB error: {e}")
        raise HTTPException(500, f"Í∂åÌïú ÌôïÏù∏ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
    
    logger.info(f"Admin access granted: {user_email}")
    return session


router = APIRouter(prefix="/admin", tags=["admin"])


@router.get("/health-check")
async def health_check_with_auto_recovery(request: Request):
    """Health check with auto-recovery for missing problems.
    
    Cloud Scheduler should call this endpoint periodically (e.g., every hour).
    If today's problems are missing, it will trigger generation automatically.
    
    This ensures data availability even if the main daily scheduler fails.
    """
    from backend.common.logging import get_logger
    import time
    logger = get_logger(__name__)
    
    # API ÌÇ§ Í≤ÄÏ¶ù (Cloud Scheduler ÎòêÎäî ÎÇ¥Î∂Ä Ìò∏Ï∂ú)
    expected_key = os.environ.get("SCHEDULER_API_KEY", "")
    provided_key = request.headers.get("X-Scheduler-Key", "")
    
    # ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏúºÎ©¥ Í≤ÄÏ¶ù, ÏïÑÎãàÎ©¥ Í±¥ÎÑàÎúÄ (dev ÌôòÍ≤Ω Ìò∏Ìôò)
    if expected_key and provided_key != expected_key:
        logger.warning("[HEALTH] Invalid API key for health check")
        raise HTTPException(403, "Invalid API key")
    
    today = get_today_kst()
    result = {
        "status": "healthy",
        "date": str(today),
        "problems_exist": False,
        "auto_generated": False,
        "details": {}
    }
    
    try:
        with postgres_connection() as pg:
            # Ïò§Îäò ÎÇ†Ïßú Î¨∏Ï†ú ÌôïÏù∏
            df = pg.fetch_df("""
                SELECT data_type, COUNT(*) as cnt 
                FROM public.problems 
                WHERE problem_date = %s 
                GROUP BY data_type
            """, [today])
            
            pa_count = 0
            for _, row in df.iterrows():
                if row.get("data_type") == "pa":
                    pa_count = int(row.get("cnt", 0))
                result["details"][row.get("data_type", "unknown")] = int(row.get("cnt", 0))
            
            if pa_count > 0:
                result["problems_exist"] = True
                logger.info(f"[HEALTH] Problems exist for {today}: {result['details']}")
                return result
            
            # Î¨∏Ï†úÍ∞Ä ÏóÜÏúºÎ©¥ ÏûêÎèô ÏÉùÏÑ± ÏãúÎèÑ
            logger.warning(f"[HEALTH] No problems found for {today}, triggering auto-generation")
            db_log(LogCategory.SCHEDULER, f"Health check ÏûêÎèô Î≥µÍµ¨: {today} Î¨∏Ï†ú ÏóÜÏùå, ÏÉùÏÑ± ÏãúÏûë", LogLevel.WARNING, "health_check")
            
            start_time = time.time()
            
            # 1. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
            try:
                from backend.generator.data_generator_advanced import generate_data
                generate_data(modes=("pa",))
                result["details"]["data_generated"] = True
            except Exception as e:
                logger.error(f"[HEALTH] Data generation failed: {e}")
                result["details"]["data_error"] = str(e)
            
            # 2. PA Î¨∏Ï†ú ÏÉùÏÑ±
            try:
                from problems.generator import generate as gen_pa
                gen_pa(today, pg)
                
                # ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ïàò ÌôïÏù∏
                new_df = pg.fetch_df("""
                    SELECT COUNT(*) as cnt FROM public.problems 
                    WHERE problem_date = %s AND data_type = 'pa'
                """, [today])
                new_count = int(new_df.iloc[0]["cnt"]) if len(new_df) > 0 else 0
                result["details"]["pa_generated"] = new_count
                result["auto_generated"] = new_count > 0
            except Exception as e:
                logger.error(f"[HEALTH] PA generation failed: {e}")
                result["details"]["pa_error"] = str(e)
            
            duration = time.time() - start_time
            result["details"]["duration_sec"] = round(duration, 2)
            
            if result["auto_generated"]:
                result["status"] = "recovered"
                db_log(LogCategory.SCHEDULER, f"Health check ÏûêÎèô Î≥µÍµ¨ ÏÑ±Í≥µ: {result['details']}", LogLevel.INFO, "health_check")
            else:
                result["status"] = "degraded"
                db_log(LogCategory.SCHEDULER, f"Health check ÏûêÎèô Î≥µÍµ¨ Ïã§Ìå®: {result['details']}", LogLevel.ERROR, "health_check")
            
            return result
            
    except Exception as e:
        logger.error(f"[HEALTH] Health check failed: {e}")
        return {
            "status": "error",
            "date": str(today),
            "error": str(e)
        }


@router.get("/status", response_model=SystemStatus)
async def get_system_status(admin=Depends(require_admin)):
    """ÏãúÏä§ÌÖú ÏÉÅÌÉú Ï°∞Ìöå"""
    from backend.common.logging import get_logger
    logger = get_logger(__name__)

    # PostgreSQL Ïó∞Í≤∞ ÌôïÏù∏
    postgres_connected = False
    tables = []
    
    try:
        with postgres_connection() as pg:
            postgres_connected = True
            
            table_df = pg.fetch_df("""
                SELECT table_name FROM information_schema.tables
                WHERE table_schema = 'public' ORDER BY table_name
            """)
            
            for _, row in table_df.iterrows():
                tbl = row["table_name"]
                try:
                    cnt = pg.fetch_df(f"SELECT COUNT(*) as cnt FROM public.{tbl}")
                    row_count = int(cnt.iloc[0]["cnt"])
                except:
                    row_count = 0
                
                col_cnt = pg.fetch_df(f"""
                    SELECT COUNT(*) as cnt FROM information_schema.columns
                    WHERE table_name = '{tbl}'
                """)
                
                tables.append(DatabaseTable(
                    table_name=tbl,
                    row_count=row_count,
                    column_count=int(col_cnt.iloc[0]["cnt"])
                ))
    except Exception as e:
        logger.error(f"Failed to fetch PostgreSQL table metadata: {e}")
    
    # DuckDB Ïó∞Í≤∞ ÌôïÏù∏
    duckdb_connected = False
    scheduler_sessions = []
    
    try:
        with duckdb_connection() as duck:
            duckdb_connected = True
            
            rows = duck.fetchall("""
                SELECT session_date, status, generated_at, problem_set_path
                FROM daily_sessions
                ORDER BY session_date DESC
                LIMIT 7
            """)
            
            scheduler_sessions = [
                SchedulerStatus(
                    session_date=r["session_date"],
                    status=r.get("status", "N/A"),
                    generated_at=r.get("generated_at"),
                    problem_set_path=r.get("problem_set_path")
                )
                for r in rows
            ]
    except Exception as e:
        logger.error(f"Failed to fetch DuckDB scheduler sessions: {e}")
    
    # Ïò§ÎäòÏùò Î¨∏Ï†ú ÌòÑÌô© ÌôïÏù∏ (DB Í∏∞Î∞ò - Cloud Run ÌååÏùº ÏãúÏä§ÌÖú ÌúòÎ∞úÏÑ± ÎåÄÏùë)
    today = get_today_kst()
    today_problems = None
    
    try:
        with postgres_connection() as pg:
            # problems ÌÖåÏù¥Î∏îÏóêÏÑú Ïò§Îäò ÎÇ†Ïßú Î¨∏Ï†ú Ï°∞Ìöå
            df = pg.fetch_df("""
                SELECT problem_id, difficulty, data_type
                FROM public.problems
                WHERE session_date = %s
            """, [today])
            
            if len(df) > 0:
                difficulties = {}
                for _, row in df.iterrows():
                    diff = row.get("difficulty", "unknown")
                    difficulties[diff] = difficulties.get(diff, 0) + 1
                
                today_problems = TodayProblemsStatus(
                    exists=True,
                    count=len(df),
                    difficulties=difficulties,
                    path=f"DB: {today.isoformat()} ({len(df)} problems)"
                )
            else:
                today_problems = TodayProblemsStatus(exists=False)
    except Exception as e:
        from backend.common.logging import get_logger
        get_logger(__name__).error(f"Error checking today problems from DB: {e}")
        today_problems = TodayProblemsStatus(exists=False)
    
    return SystemStatus(
        postgres_connected=postgres_connected,
        duckdb_connected=duckdb_connected,
        tables=tables,
        scheduler_sessions=scheduler_sessions,
        today_problems=today_problems
    )



@router.post("/generate-problems", response_model=GenerateProblemsResponse)
async def generate_problems(request: GenerateProblemsRequest, admin=Depends(require_admin)):
    """Î¨∏Ï†ú ÏÉùÏÑ± (PA ÎòêÎäî Stream)"""
    today = get_today_kst()
    
    if request.data_type in ["pa", "rca"]:
        try:
            from problems.generator import generate as gen_problems
            mode = request.data_type
            
            with postgres_connection() as pg:
                path = gen_problems(today, pg, mode=mode)
            
            # ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ïàò ÌôïÏù∏
            import json
            with open(path, encoding="utf-8") as f:
                data = json.load(f)
                # ÏõîÎ≥Ñ ÌååÏùºÏùº Í≤ΩÏö∞ Ï†ÑÏ≤¥Í∞Ä ÏïÑÎãå Î∞©Í∏à ÏÉùÏÑ±Îêú Î¨∏Ï†úÎßå ÏÑ∏Îäî Í≤ÉÏù¥ Ï†ïÌôïÌïòÏßÄÎßå, 
                # ÏùºÎã® ÌååÏùº ÏãúÏä§ÌÖú Íµ¨Ï°∞ÏÉÅ daily/_today.jsonÏùÑ ÌôïÏù∏ÌïòÎäî Í≤ÉÏù¥ Îçî Îπ†Î•º Ïàò ÏûàÏùå
                import os
                daily_filename = f"{mode}_{today}.json" if mode != "pa" else f"{today}.json"
                daily_path = os.path.join("problems/daily", daily_filename)
                if os.path.exists(daily_path):
                    with open(daily_path, encoding="utf-8") as df:
                        problems = json.load(df)
                        p_count = len(problems)
                else:
                    problems = data.get("problems", []) if isinstance(data, dict) else data
                    p_count = len(problems)
            
            return GenerateProblemsResponse(
                success=True,
                message=f"{mode.upper()} Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å",
                path=path,
                problem_count=p_count
            )
        except Exception as e:
            return GenerateProblemsResponse(
                success=False,
                message=f"{request.data_type.upper()} Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"
            )
    
    elif request.data_type == "stream":
        try:
            from problems.generator_stream import generate_stream_problems
            
            with postgres_connection() as pg:
                path = generate_stream_problems(today, pg)
            
            import json
            with open(path, encoding="utf-8") as f:
                problems = json.load(f)
            
            return GenerateProblemsResponse(
                success=True,
                message="Stream Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å",
                path=path,
                problem_count=len(problems)
            )
        except Exception as e:
            return GenerateProblemsResponse(
                success=False,
                message=f"Stream Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"
            )
    
    else:
        return GenerateProblemsResponse(
            success=False,
            message="ÏßÄÏõêÌïòÏßÄ ÏïäÎäî data_typeÏûÖÎãàÎã§."
        )


@router.post("/trigger-rca", response_model=GenerateProblemsResponse)
async def trigger_rca(request: TriggerRCARequest, admin=Depends(require_admin)):
    """RCA ÏãúÎÇòÎ¶¨Ïò§ ÏàòÎèô Ìä∏Î¶¨Í±∞ (Ïù¥ÏÉÅ Îç∞Ïù¥ÌÑ∞ Ï£ºÏûÖ + Î¨∏Ï†ú ÏÉùÏÑ±)"""
    from backend.common.logging import get_logger
    logger = get_logger(__name__)
    today = get_today_kst()
    
    try:
        from backend.generator.anomaly_injector import inject_random_anomaly, AnomalyType
        from backend.generator.data_generator_advanced import PRODUCT_PROFILES
        
        # 1. Ïù¥ÏÉÅ Ìå®ÌÑ¥ Ï£ºÏûÖ
        p_type = request.product_type or "commerce"
        if p_type not in PRODUCT_PROFILES:
            return GenerateProblemsResponse(success=False, message=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî product_type: {p_type}")
        
        a_type = None
        if request.anomaly_type:
            try:
                a_type = AnomalyType(request.anomaly_type)
            except ValueError:
                return GenerateProblemsResponse(success=False, message=f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî anomaly_type: {request.anomaly_type}")
        
        logger.info(f"[RCA] Triggering anomaly injection: product={p_type}, type={a_type}")
        with postgres_connection() as pg:
            anomaly_info = inject_random_anomaly(pg, p_type, force_type=a_type)
            
            if not anomaly_info:
                return GenerateProblemsResponse(success=False, message="Ïù¥ÏÉÅ Ìå®ÌÑ¥ Ï£ºÏûÖ Ïã§Ìå®")
            
            # 2. RCA Î¨∏Ï†ú ÏÉùÏÑ± (Ï£ºÏûÖÎêú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä Ï∞∏Ï°∞Ìï®)
            from problems.generator import generate as gen_problems
            path = gen_problems(today, pg, mode="rca", product_type=p_type)
            
            # ÏÉùÏÑ±Îêú Î¨∏Ï†ú Ïàò ÌôïÏù∏
            import json
            with open(path, encoding="utf-8") as f:
                data = json.load(f)
                problems = data.get("problems", []) if isinstance(data, dict) else data
                p_count = len(problems)
            
            return GenerateProblemsResponse(
                success=True,
                message=f"RCA Ìä∏Î¶¨Í±∞ ÏôÑÎ£å: {anomaly_info['type']} Ï£ºÏûÖ Î∞è Î¨∏Ï†ú {p_count}Í∞ú ÏÉùÏÑ±",
                path=path,
                problem_count=p_count
            )
            
    except Exception as e:
        logger.error(f"[RCA] Trigger failed: {e}")
        return GenerateProblemsResponse(success=False, message=f"RCA Ìä∏Î¶¨Í±∞ Ïã§Ìå®: {str(e)}")


@router.post("/refresh-data", response_model=RefreshDataResponse)
async def refresh_data(request: RefreshDataRequest, admin=Depends(require_admin)):
    """Îç∞Ïù¥ÌÑ∞ Í∞±Ïã†"""
    try:
        from backend.generator.data_generator_advanced import generate_data
        
        if request.data_type == "pa":
            generate_data(modes=("pa",))
            return RefreshDataResponse(success=True, message="PA Îç∞Ïù¥ÌÑ∞ Í∞±Ïã† ÏôÑÎ£å")
        elif request.data_type == "stream":
            generate_data(modes=("stream",))
            return RefreshDataResponse(success=True, message="Stream Îç∞Ïù¥ÌÑ∞ Í∞±Ïã† ÏôÑÎ£å")
        else:
            return RefreshDataResponse(success=False, message="ÏûòÎ™ªÎêú data_type")
    except Exception as e:
        return RefreshDataResponse(success=False, message=f"Îç∞Ïù¥ÌÑ∞ Í∞±Ïã† Ïã§Ìå®: {str(e)}")






@router.post("/initial-setup")
async def initial_setup(admin=Depends(require_admin)):
    """Ï¥àÍ∏∞ Îç∞Ïù¥ÌÑ∞ ÏÖãÏóÖ (PA + Stream Îç∞Ïù¥ÌÑ∞ Î∞è Î¨∏Ï†ú ÏÉùÏÑ±)"""
    results = []
    errors = []
    
    try:
        # 1. PA Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        try:
            from backend.generator.data_generator_advanced import generate_data
            generate_data(modes=("pa",))
            results.append("‚úì PA Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å")
        except Exception as e:
            errors.append(f"‚úó PA Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        # 2. Stream Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        try:
            from backend.generator.data_generator_advanced import generate_data
            generate_data(modes=("stream",))
            results.append("‚úì Stream Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å")
        except Exception as e:
            errors.append(f"‚úó Stream Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        # 3. PA Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator import generate as gen_problems
            with postgres_connection() as pg:
                path = gen_problems(get_today_kst(), pg)
            results.append(f"‚úì PA Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å: {path}")
        except Exception as e:
            errors.append(f"‚úó PA Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        # 4. Stream Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator_stream import generate_stream_problems
            with postgres_connection() as pg:
                path = generate_stream_problems(get_today_kst(), pg)
            results.append(f"‚úì Stream Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å: {path}")
        except Exception as e:
            errors.append(f"‚úó Stream Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        db_log(
            category=LogCategory.SYSTEM,
            message=f"Initial setup completed. Success: {len(results)}, Errors: {len(errors)}",
            level=LogLevel.INFO if not errors else LogLevel.WARNING,
            source="admin"
        )
        
        return {
            "success": len(errors) == 0,
            "results": results,
            "errors": errors,
            "message": f"{len(results)}Í∞ú ÏûëÏóÖ ÏôÑÎ£å, {len(errors)}Í∞ú Ïã§Ìå®"
        }
    except Exception as e:
        return {
            "success": False,
            "results": results,
            "errors": errors + [f"Unexpected error: {str(e)}"],
            "message": "Ï¥àÍ∏∞Ìôî Ï§ë Ïò§Î•ò Î∞úÏÉù"
        }


@router.post("/trigger-now")
async def trigger_generation_now(admin=Depends(require_admin)):
    """Ï¶âÏãú Î¨∏Ï†ú/Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìñâ (Ïä§ÏºÄÏ§ÑÎü¨ ÏàòÎèô Ìä∏Î¶¨Í±∞)"""
    results = []
    errors = []
    
    from datetime import date
    today = get_today_kst()
    
    try:
        # 1. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (PA + Stream)
        try:
            from backend.generator.data_generator_advanced import generate_data
            generate_data(modes=("pa", "stream"))
            results.append("‚úì PA/Stream Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏôÑÎ£å")
        except Exception as e:
            errors.append(f"‚úó Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        # 2. PA Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator import generate as gen_pa
            with postgres_connection() as pg:
                path = gen_pa(today, pg)
            results.append(f"‚úì PA Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å: {path}")
        except Exception as e:
            errors.append(f"‚úó PA Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        # 3. Stream Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator_stream import generate_stream_problems
            with duckdb_connection() as duck:
                path = generate_stream_problems(target_date=today, duck=duck)
            results.append(f"‚úì Stream Î¨∏Ï†ú ÏÉùÏÑ± ÏôÑÎ£å: {path}")
        except Exception as e:
            errors.append(f"‚úó Stream Î¨∏Ï†ú ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
        
        return {
            "success": len(errors) == 0,
            "date": today.isoformat(),
            "results": results,
            "errors": errors,
            "message": f"ÏôÑÎ£å ({len(results)}Í∞ú ÏÑ±Í≥µ, {len(errors)}Í∞ú Ïã§Ìå®)"
        }
    except Exception as e:
        return {
            "success": False,
            "results": results,
            "errors": errors + [str(e)],
            "message": "ÏòàÍ∏∞Ïπò ÏïäÏùÄ Ïò§Î•ò"
        }

@router.get("/dataset-versions")
async def get_dataset_versions():
    """Îç∞Ïù¥ÌÑ∞ÏÖã/Î¨∏Ï†ú ÏÉùÏÑ± Ïù¥Î†• Ï°∞Ìöå"""
    try:
        with postgres_connection() as pg:
            df = pg.fetch_df("""
                SELECT 
                    version_id,
                    created_at,
                    generation_date,
                    generation_type,
                    data_type,
                    problem_count,
                    n_users,
                    n_events,
                    status,
                    error_message,
                    duration_ms
                FROM public.dataset_versions
                ORDER BY created_at DESC
                LIMIT 30
            """)
            versions = []
            for _, row in df.iterrows():
                versions.append({
                    "version_id": row.get("version_id"),
                    "created_at": row.get("created_at").isoformat() if row.get("created_at") else None,
                    "generation_date": row.get("generation_date").isoformat() if row.get("generation_date") else None,
                    "generation_type": row.get("generation_type"),
                    "data_type": row.get("data_type"),
                    "problem_count": row.get("problem_count"),
                    "n_users": row.get("n_users"),
                    "n_events": row.get("n_events"),
                    "status": row.get("status"),
                    "error_message": row.get("error_message"),
                    "duration_ms": row.get("duration_ms")
                })
            return {"success": True, "versions": versions}
    except Exception as e:
        return {"success": False, "message": str(e), "versions": []}


@router.get("/problem-files")
async def get_problem_files(admin=Depends(require_admin)):
    """Î¨∏Ï†ú ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
    import os
    from pathlib import Path
    from datetime import datetime
    import json
    
    problem_dir = Path("/app/problems/daily")
    if not problem_dir.exists():
        problem_dir = Path("problems/daily")
    
    files = []
    
    try:
        for f in sorted(problem_dir.glob("*.json"), reverse=True):
            stat = f.stat()
            
            # Î¨∏Ï†ú Í∞úÏàò ÌôïÏù∏
            try:
                with open(f, 'r') as fp:
                    data = json.load(fp)
                    problem_count = len(data) if isinstance(data, list) else 1
            except:
                problem_count = 0
            
            # ÌååÏùº ÌÉÄÏûÖ Íµ¨Î∂Ñ
            file_type = "stream" if f.name.startswith("stream_") else "pa"
            
            files.append({
                "filename": f.name,
                "type": file_type,
                "size_kb": round(stat.st_size / 1024, 1),
                "problem_count": problem_count,
                "created_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            })
        
        # ÏµúÍ∑º 30Í∞úÎßå Î∞òÌôò
        return {"success": True, "files": files[:30]}
    except Exception as e:
        return {"success": False, "message": str(e), "files": []}


@router.get("/scheduler-logs")
async def get_scheduler_logs(lines: int = 50):
    """Ïä§ÏºÄÏ§ÑÎü¨ Î°úÍ∑∏ Ï°∞Ìöå (docker logs)"""
    import subprocess
    try:
        result = subprocess.run(
            ["docker", "compose", "logs", "scheduler", "--tail", str(lines)],
            capture_output=True,
            text=True,
            timeout=10,
            cwd="/app"  # docker composeÍ∞Ä Ïã§ÌñâÎêòÎäî ÏúÑÏπò
        )
        logs = result.stdout or result.stderr
        
        # Î°úÍ∑∏Î•º Ï§Ñ Îã®ÏúÑÎ°ú ÌååÏã±
        log_lines = []
        for line in logs.split('\n'):
            if 'scheduler-1' in line:
                # "scheduler-1  | " Ï†úÍ±∞
                clean_line = line.split('|', 1)[-1].strip() if '|' in line else line
                log_lines.append(clean_line)
        
        return {
            "success": True, 
            "logs": log_lines[-lines:],  # ÏµúÍ∑º NÏ§Ñ
            "total_lines": len(log_lines)
        }
    except subprocess.TimeoutExpired:
        return {"success": False, "message": "Î°úÍ∑∏ Ï°∞Ìöå ÌÉÄÏûÑÏïÑÏõÉ", "logs": []}
    except Exception as e:
        return {"success": False, "message": str(e), "logs": []}


@router.get("/scheduler-status")
async def get_scheduler_status_admin(admin=Depends(require_admin)):
    """Ïä§ÏºÄÏ§ÑÎü¨ ÏÉÅÌÉú Ï°∞Ìöå (ÎÇ¥Î∂Ä Ïä§ÏºÄÏ§ÑÎü¨)"""
    try:
        from backend.scheduler import get_scheduler_status, scheduler
        status = get_scheduler_status()
        
        # Ï∂îÍ∞Ä Ï†ïÎ≥¥
        jobs_info = []
        for job in scheduler.get_jobs():
            next_run = job.next_run_time
            jobs_info.append({
                "id": job.id,
                "name": job.name or job.id,
                "next_run": next_run.strftime("%Y-%m-%d %H:%M:%S KST") if next_run else "ÎØ∏Ï†ï",
                "trigger": str(job.trigger)
            })
        
        return {
            "success": True,
            "running": status["running"],
            "jobs": jobs_info,
            "last_run_times": status["last_run_times"]
        }
    except Exception as e:
        return {"success": False, "message": str(e), "running": False, "jobs": []}


@router.post("/run-scheduler-job")
async def run_scheduler_job(job_type: str, admin=Depends(require_admin)):
    """Ïä§ÏºÄÏ§ÑÎü¨ ÏûëÏóÖ ÏàòÎèô Ïã§Ìñâ"""
    try:
        if job_type == "weekday":
            from backend.scheduler import run_weekday_generation
            run_weekday_generation()
            return {"success": True, "message": "ÌèâÏùº Î¨∏Ï†ú/Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏûëÏóÖ Ïã§ÌñâÎê®"}
        elif job_type == "sunday":
            from backend.scheduler import run_sunday_generation
            run_sunday_generation()
            return {"success": True, "message": "ÏùºÏöîÏùº Stream Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏûëÏóÖ Ïã§ÌñâÎê®"}
        elif job_type == "cleanup":
            from backend.scheduler import cleanup_old_data
            cleanup_old_data()
            return {"success": True, "message": "Îç∞Ïù¥ÌÑ∞ Ï†ïÎ¶¨ ÏûëÏóÖ Ïã§ÌñâÎê®"}
        else:
            return {"success": False, "message": f"Ïïå Ïàò ÏóÜÎäî ÏûëÏóÖ ÌÉÄÏûÖ: {job_type}"}
    except Exception as e:
        return {"success": False, "message": str(e)}


@router.post("/reset-submissions")
async def reset_submissions(admin=Depends(require_admin)):
    """Ï†úÏ∂ú Í∏∞Î°ù Ï¥àÍ∏∞Ìôî Î∞è XP Î¶¨ÏÖã"""
    from backend.common.logging import get_logger
    logger = get_logger(__name__)

    try:
        with postgres_connection() as pg:
            pg.execute("TRUNCATE TABLE public.submissions RESTART IDENTITY")
            pg.execute("UPDATE public.users SET xp = 0, level = 1")
        
        # DuckDBÏùò Î∂ÑÏÑùÏö© Îç∞Ïù¥ÌÑ∞ÎèÑ ÏÇ≠Ï†ú
        try:
            with duckdb_connection() as duck:
                duck.execute("DELETE FROM pa_submissions")
        except Exception as e:
            logger.warning(f"Failed to delete DuckDB submissions during reset: {e}")
        
        return {
            "success": True,
            "message": "Î™®Îì† Ï†úÏ∂ú Í∏∞Î°ùÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏóàÏäµÎãàÎã§."
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {str(e)}"
        }

@router.get("/logs")
async def get_system_logs(
    admin=Depends(require_admin),
    category: Optional[str] = Query(None, description="Î°úÍ∑∏ Ïπ¥ÌÖåÍ≥†Î¶¨"),
    level: Optional[str] = Query(None, description="Î°úÍ∑∏ Î†àÎ≤®"),
    limit: int = Query(100, description="Ï°∞Ìöå Í∞úÏàò")
):
    """ÏãúÏä§ÌÖú Î°úÍ∑∏ Ï°∞Ìöå"""
    logs = get_logs(category=category, level=level, limit=limit)
    return {
        "success": True,
        "logs": logs,
        "count": len(logs)
    }


@router.get("/log-categories")
async def get_log_categories(admin=Depends(require_admin)):
    """Î°úÍ∑∏ Ïπ¥ÌÖåÍ≥†Î¶¨ Î™©Î°ù"""
    return {
        "categories": [
            {"id": "problem_generation", "name": "Î¨∏Ï†ú ÏÉùÏÑ±", "icon": "ü§ñ"},
            {"id": "user_action", "name": "ÏÇ¨Ïö©Ïûê Ïï°ÏÖò", "icon": "üë§"},
            {"id": "scheduler", "name": "Ïä§ÏºÄÏ§ÑÎü¨", "icon": "‚è∞"},
            {"id": "system", "name": "ÏãúÏä§ÌÖú", "icon": "üñ•Ô∏è"},
            {"id": "api", "name": "API", "icon": "üîå"}
        ]
    }


@router.get("/users")
async def get_all_users(admin=Depends(require_admin)):
    """Ï†ÑÏ≤¥ ÏÇ¨Ïö©Ïûê Î™©Î°ù Ï°∞Ìöå"""
    try:
        with postgres_connection() as pg:
            df = pg.fetch_df("""
                SELECT id, email, name, nickname, xp, level, is_admin, created_at
                FROM public.users
                ORDER BY created_at DESC
            """)
            users = []
            for _, row in df.iterrows():
                users.append({
                    "id": row["id"],
                    "email": row["email"],
                    "name": row["name"],
                    "nickname": row["nickname"],
                    "xp": int(row.get("xp", 0)),
                    "level": int(row.get("level", 1)),
                    "is_admin": bool(row.get("is_admin", False)),
                    "created_at": row["created_at"].isoformat() if row["created_at"] else None
                })
            return {"success": True, "users": users, "count": len(users)}
    except Exception as e:
        return {"success": False, "message": str(e), "users": []}


@router.patch("/users/{user_id}/admin")
async def toggle_user_admin(user_id: str, admin=Depends(require_admin)):
    """ÏÇ¨Ïö©Ïûê Í¥ÄÎ¶¨Ïûê Í∂åÌïú ÌÜ†Í∏Ä"""
    try:
        with postgres_connection() as pg:
            # ÌòÑÏû¨ ÏÉÅÌÉú Ï°∞Ìöå
            df = pg.fetch_df("SELECT is_admin FROM public.users WHERE id = %s", [user_id])
            if len(df) == 0:
                return {"success": False, "message": "ÏÇ¨Ïö©ÏûêÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"}
            
            current_admin = bool(df.iloc[0].get("is_admin", False))
            new_admin = not current_admin
            
            pg.execute("UPDATE public.users SET is_admin = %s WHERE id = %s", [new_admin, user_id])
            
            db_log(
                category=LogCategory.SYSTEM,
                message=f"ÏÇ¨Ïö©Ïûê Í¥ÄÎ¶¨Ïûê Í∂åÌïú Î≥ÄÍ≤Ω: {user_id} -> {'Í¥ÄÎ¶¨Ïûê' if new_admin else 'ÏùºÎ∞ò'}",
                level=LogLevel.INFO,
                source="admin_api"
            )
            
            return {"success": True, "is_admin": new_admin}
    except Exception as e:
        return {"success": False, "message": str(e)}


@router.delete("/users/{user_id}")
async def delete_user(user_id: str, admin=Depends(require_admin)):
    """ÏÇ¨Ïö©Ïûê ÏÇ≠Ï†ú"""
    try:
        with postgres_connection() as pg:
            pg.execute("DELETE FROM public.submissions WHERE user_id = %s", [user_id])
            pg.execute("DELETE FROM public.user_problem_sets WHERE user_id = %s", [user_id])
            pg.execute("DELETE FROM public.users WHERE id = %s", [user_id])
            
            db_log(
                category=LogCategory.SYSTEM,
                message=f"ÏÇ¨Ïö©Ïûê ÏÇ≠Ï†ú: {user_id}",
                level=LogLevel.WARNING,
                source="admin_api"
            )
            
            return {"success": True, "message": "ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§"}
    except Exception as e:
        return {"success": False, "message": str(e)}


@router.get("/api-usage")
async def get_api_usage(
    admin=Depends(require_admin),
    limit: int = Query(100, description="Ï°∞ÌöåÌï† Î°úÍ∑∏ Ïàò"),
    days: int = Query(7, description="ÏµúÍ∑º NÏùº")
):
    """Gemini API ÏÇ¨Ïö©Îüâ Ï°∞Ìöå"""
    try:
        with postgres_connection() as pg:
            # ÌÖåÏù¥Î∏î Ï°¥Ïû¨ ÌôïÏù∏
            pg.execute("""
                CREATE TABLE IF NOT EXISTS public.api_usage_logs (
                    id SERIAL PRIMARY KEY,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    purpose VARCHAR(100),
                    model VARCHAR(50),
                    input_tokens INTEGER DEFAULT 0,
                    output_tokens INTEGER DEFAULT 0,
                    total_tokens INTEGER DEFAULT 0,
                    user_id VARCHAR(100)
                )
            """)
            
            # ÏùºÎ≥Ñ ÏÇ¨Ïö©Îüâ
            daily_df = pg.fetch_df("""
                SELECT 
                    DATE(timestamp) as date,
                    purpose,
                    COUNT(*) as call_count,
                    SUM(total_tokens) as total_tokens
                FROM public.api_usage_logs
                WHERE timestamp >= CURRENT_DATE - INTERVAL '%s days'
                GROUP BY DATE(timestamp), purpose
                ORDER BY date DESC, purpose
            """, [days])
            
            # ÏµúÍ∑º Î°úÍ∑∏
            logs_df = pg.fetch_df("""
                SELECT 
                    timestamp, purpose, model, input_tokens, output_tokens, total_tokens, user_id
                FROM public.api_usage_logs
                ORDER BY timestamp DESC
                LIMIT %s
            """, [limit])
            
            # Ï¥ùÍ≥Ñ
            total_df = pg.fetch_df("""
                SELECT 
                    COUNT(*) as total_calls,
                    COALESCE(SUM(total_tokens), 0) as total_tokens,
                    COALESCE(SUM(input_tokens), 0) as input_tokens,
                    COALESCE(SUM(output_tokens), 0) as output_tokens
                FROM public.api_usage_logs
                WHERE timestamp >= CURRENT_DATE - INTERVAL '%s days'
            """, [days])
            
            # Gemini Í∞ÄÍ≤© Í≥ÑÏÇ∞ (ÎåÄÎûµÏ†Å - 1.5 Pro Í∏∞Ï§Ä $0.00025/1K input, $0.0005/1K output)
            total_input = int(total_df.iloc[0]['input_tokens']) if len(total_df) > 0 else 0
            total_output = int(total_df.iloc[0]['output_tokens']) if len(total_df) > 0 else 0
            estimated_cost_usd = (total_input / 1000 * 0.00025) + (total_output / 1000 * 0.0005)
            
            return {
                "summary": {
                    "total_calls": int(total_df.iloc[0]['total_calls']) if len(total_df) > 0 else 0,
                    "total_tokens": int(total_df.iloc[0]['total_tokens']) if len(total_df) > 0 else 0,
                    "input_tokens": total_input,
                    "output_tokens": total_output,
                    "estimated_cost_usd": round(estimated_cost_usd, 4),
                    "period_days": days
                },
                "daily": daily_df.to_dict('records') if len(daily_df) > 0 else [],
                "logs": logs_df.to_dict('records') if len(logs_df) > 0 else []
            }
    except Exception as e:
        return {"error": str(e), "summary": {}, "daily": [], "logs": []}


# ============================================
# Cloud Scheduler Ìä∏Î¶¨Í±∞ ÏóîÎìúÌè¨Ïù∏Ìä∏
# ============================================

@router.post("/trigger/daily-generation")
async def trigger_daily_generation(request: Request):
    """Cloud SchedulerÏö© ÏùºÏùº Îç∞Ïù¥ÌÑ∞/Î¨∏Ï†ú ÏÉùÏÑ± Ìä∏Î¶¨Í±∞
    
    Ìó§ÎçîÏóê X-Scheduler-KeyÍ∞Ä SCHEDULER_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàòÏôÄ ÏùºÏπòÌï¥Ïïº Ìï®
    """
    from backend.common.logging import get_logger
    logger = get_logger(__name__)
    
    # ÏöîÏ≤≠ Ï†ïÎ≥¥ Î°úÍπÖ (ÎîîÎ≤ÑÍπÖÏö©)
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("User-Agent", "unknown")
    logger.info(f"[TRIGGER] Request received from IP={client_ip}, UA={user_agent[:50]}")
    
    # API ÌÇ§ Í≤ÄÏ¶ù
    expected_key = os.environ.get("SCHEDULER_API_KEY", "")
    provided_key = request.headers.get("X-Scheduler-Key", "")
    
    if not expected_key:
        logger.warning("[TRIGGER] SCHEDULER_API_KEY not configured in environment")
        raise HTTPException(403, "Scheduler API key not configured")
    
    if provided_key != expected_key:
        logger.warning(f"[TRIGGER] Invalid API key provided (length={len(provided_key)})")
        raise HTTPException(403, "Invalid API key")
    
    logger.info("[TRIGGER] API key validated, starting daily generation")
    db_log(LogCategory.SCHEDULER, "Cloud Scheduler Ìä∏Î¶¨Í±∞ ÏàòÏã† (Ïù∏Ï¶ù ÏÑ±Í≥µ)", LogLevel.INFO, "trigger")

    # KST Í∏∞Ï§Ä Ïò§Îäò ÎÇ†Ïßú Í≥ÑÏÇ∞ (GCP ÌôòÍ≤Ω ÎåÄÎπÑ)
    today = get_today_kst()
    logger.info(f"[TRIGGER] Target date: {today}")

    # Ï§ëÎ≥µ Ïã§Ìñâ Î∞©ÏßÄ: Ïò§Îäò ÎÇ†ÏßúÎ°ú Ïù¥ÎØ∏ ÏÉùÏÑ±ÎêòÏóàÎäîÏßÄ ÌôïÏù∏
    try:
        from backend.services.database import postgres_connection
        with postgres_connection() as pg:
            # PAÏôÄ Stream Î¨∏Ï†ú Î™®Îëê ÌôïÏù∏
            existing = pg.fetch_df("""
                SELECT data_type, COUNT(*) as cnt 
                FROM public.problems 
                WHERE session_date = %s 
                GROUP BY data_type
            """, [today])
            
            pa_count = 0
            stream_count = 0
            for _, row in existing.iterrows():
                if row.get("data_type") == "pa":
                    pa_count = int(row.get("cnt", 0))
                elif row.get("data_type") == "stream":
                    stream_count = int(row.get("cnt", 0))
            
            if pa_count > 0 and stream_count > 0:
                logger.info(f"[TRIGGER] Problems already exist for {today}: PA={pa_count}, Stream={stream_count}")
                db_log(LogCategory.SCHEDULER, f"{today} Ï§ëÎ≥µ ÏÉùÏÑ± Î∞©ÏßÄ (PA={pa_count}, Stream={stream_count})", LogLevel.INFO, "trigger")
                return {
                    "status": "already_generated",
                    "date": str(today),
                    "pa_count": pa_count,
                    "stream_count": stream_count,
                    "message": f"{today} ÎÇ†ÏßúÏùò Î¨∏Ï†úÍ∞Ä Ïù¥ÎØ∏ ÏÉùÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
                }
            elif pa_count > 0 or stream_count > 0:
                logger.info(f"[TRIGGER] Partial generation detected for {today}: PA={pa_count}, Stream={stream_count}")
    except Exception as e:
        logger.warning(f"[TRIGGER] Duplicate check failed (continuing anyway): {e}")

    results = {"date": str(today), "pa_data": False, "stream_data": False, "pa_problems": False, "stream_problems": False}
    
    try:
        from backend.engine.postgres_engine import PostgresEngine
        from backend.config.db import PostgresEnv
        pg = PostgresEngine(PostgresEnv().dsn())
        
        # 1. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        try:
            from backend.generator.data_generator_advanced import generate_data
            generate_data(modes=("pa", "stream"))
            results["pa_data"] = True
            results["stream_data"] = True
            logger.info("[TRIGGER] Data generation done")
        except Exception as e:
            logger.warning(f"[TRIGGER] Data gen error: {e}")
        
        # 2. PA Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator import generate as gen_pa
            gen_pa(today, pg)
            results["pa_problems"] = True
            logger.info("[TRIGGER] PA problems generated")
        except Exception as e:
            logger.warning(f"[TRIGGER] PA problem gen error: {e}")
        
        # 3. Stream Î¨∏Ï†ú ÏÉùÏÑ±
        try:
            from problems.generator_stream import generate_stream_problems
            generate_stream_problems(today, pg)
            results["stream_problems"] = True
            logger.info("[TRIGGER] Stream problems generated")
        except Exception as e:
            logger.warning(f"[TRIGGER] Stream problem gen error: {e}")
        
        # 4. Ïò§ÎäòÏùò ÌåÅ ÏÉùÏÑ±
        try:
            from backend.services.tip_service import generate_tip_of_the_day
            generate_tip_of_the_day(today)
            results["daily_tip"] = True
            logger.info("[TRIGGER] Daily tip generated")
        except Exception as e:
            logger.warning(f"[TRIGGER] Tip gen error: {e}")
        
        db_log(LogCategory.SCHEDULER, f"ÏùºÏùº ÏÉùÏÑ± ÏôÑÎ£å: {results}", LogLevel.INFO, "trigger")
        
    except Exception as e:
        logger.error(f"[TRIGGER] Fatal error: {e}")
        results["error"] = str(e)
        
        # [AI Doctor] ÏπòÎ™ÖÏ†Å ÏóêÎü¨ Î∞úÏÉù Ïãú ÏûêÎèô ÏßÑÎã® Î∞è Î≥µÍµ¨ ÏãúÎèÑ
        try:
            from backend.services.ai_doctor import AIDoctor, send_doctor_report
            doctor = AIDoctor()
            diagnosis_report = doctor.diagnose_and_heal(e, results)
            send_doctor_report(diagnosis_report, str(today))
            logger.info("[AI Doctor] Diagnosis and report sent.")
            results["ai_diagnosis"] = diagnosis_report
        except Exception as doctor_err:
            logger.error(f"[AI Doctor] Failed to diagnose: {doctor_err}")
    
    # 5. Í≤∞Í≥º Ïù¥Î©îÏùº ÏïåÎ¶º Î∞úÏÜ° (Ï†ïÏÉÅ ÎòêÎäî ÏùºÎ∂Ä Ïã§Ìå® Ïãú)
    if not results.get("ai_diagnosis"): # AI DoctorÍ∞Ä Ï≤òÎ¶¨ÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞Îßå ÏùºÎ∞ò Ïù¥Î©îÏùº Î∞úÏÜ°
        try:
            from backend.services.notification_service import send_email
            status_str = "ÏÑ±Í≥µ" if results.get("pa_problems") and results.get("stream_problems") else "ÏùºÎ∂Ä Ïã§Ìå®"
            if "error" in results:
                status_str = "ÏπòÎ™ÖÏ†Å Ïã§Ìå®"
                
            subject = f"ÏùºÏùº Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Í≤∞Í≥º: {status_str} ({results['date']})"
            body = f"ÏùºÏùº Îç∞Ïù¥ÌÑ∞ Î∞è Î¨∏Ï†ú ÏÉùÏÑ± Í≤∞Í≥º Î≥¥Í≥†ÏûÖÎãàÎã§.\n\n"
            body += f"- ÎÇ†Ïßú: {results['date']}\n"
            body += f"- PA Îç∞Ïù¥ÌÑ∞: {'‚úÖ' if results.get('pa_data') else '‚ùå'}\n"
            body += f"- Stream Îç∞Ïù¥ÌÑ∞: {'‚úÖ' if results.get('stream_data') else '‚ùå'}\n"
            body += f"- PA Î¨∏Ï†ú: {'‚úÖ' if results.get('pa_problems') else '‚ùå'}\n"
            body += f"- Stream Î¨∏Ï†ú: {'‚úÖ' if results.get('stream_problems') else '‚ùå'}\n"
            body += f"- Ïò§ÎäòÏùò ÌåÅ: {'‚úÖ' if results.get('daily_tip') else '‚ùå'}\n"
            
            if "error" in results:
                body += f"\n[ÏóêÎü¨ Î∞úÏÉù]\n{results['error']}\n"
                
            send_email(subject, body)
        except Exception as email_err:
            logger.error(f"Failed to send result email: {email_err}")
    
    return results


@router.post("/trigger/daily-tip")
async def trigger_daily_tip(request: Request):
    """Ïò§ÎäòÏùò ÌåÅ ÏÉùÏÑ± Ìä∏Î¶¨Í±∞ (ÎèÖÎ¶Ω Ìò∏Ï∂úÏö©)"""
    expected_key = os.environ.get("SCHEDULER_API_KEY", "")
    provided_key = request.headers.get("X-Scheduler-Key", "")

    if not expected_key or provided_key != expected_key:
        raise HTTPException(403, "Invalid API key")

    from backend.services.tip_service import generate_tip_of_the_day
    today = get_today_kst()
    return generate_tip_of_the_day(today)


@router.post("/schedule/run", response_model=ScheduleRunResponse)
async def run_scheduled_generation(request: Request):
    """Cloud SchedulerÏö© ÏóîÎìúÌè¨Ïù∏Ìä∏ - Worker Job Ìä∏Î¶¨Í±∞

    Cloud Run JobÏùÑ ÎπÑÎèôÍ∏∞Î°ú Ïã§ÌñâÌïòÍ≥† Ï¶âÏãú Î∞òÌôòÌï©ÎãàÎã§.
    Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞/Î¨∏Ï†ú ÏÉùÏÑ±ÏùÄ Worker JobÏóêÏÑú ÏàòÌñâÎê©ÎãàÎã§.
    """
    import time
    import subprocess
    from backend.common.logging import get_logger
    logger = get_logger(__name__)

    start_time = time.time()

    # API ÌÇ§ Í≤ÄÏ¶ù
    expected_key = os.environ.get("SCHEDULER_API_KEY", "")
    provided_key = request.headers.get("X-Scheduler-Key", "")

    if not expected_key:
        logger.warning("[SCHEDULE/RUN] SCHEDULER_API_KEY not configured")
        raise HTTPException(403, "Scheduler API key not configured")

    if provided_key != expected_key:
        logger.warning(f"[SCHEDULE/RUN] Invalid API key (length={len(provided_key)})")
        raise HTTPException(403, "Invalid API key")

    logger.info("[SCHEDULE/RUN] API key validated, triggering Worker Job")

    today = get_today_kst()
    
    # Ï§ëÎ≥µ Ïã§Ìñâ Î∞©ÏßÄ ÌôïÏù∏
    try:
        with postgres_connection() as pg:
            existing = pg.fetch_df("""
                SELECT data_type, COUNT(*) as cnt
                FROM public.problems
                WHERE problem_date = %s
                GROUP BY data_type
            """, [today])

            pa_count = 0
            for _, row in existing.iterrows():
                if row.get("data_type") == "pa":
                    pa_count = int(row.get("cnt", 0))

            if pa_count > 0:
                duration = time.time() - start_time
                logger.info(f"[SCHEDULE/RUN] Already generated for {today}")
                return ScheduleRunResponse(
                    status="already_generated",
                    data_generated=True,
                    problems_generated=pa_count,
                    duration_sec=round(duration, 2),
                    date=str(today),
                    details={"pa_count": pa_count, "message": "Problems already exist for today"}
                )
    except Exception as e:
        logger.warning(f"[SCHEDULE/RUN] Duplicate check failed: {e}")

    # Cloud Run Job Ìä∏Î¶¨Í±∞ (ÎπÑÎèôÍ∏∞)
    project_id = os.environ.get("GOOGLE_CLOUD_PROJECT", "querycraft-483512")
    region = os.environ.get("CLOUD_RUN_REGION", "us-central1")
    job_name = "querycraft-worker"
    
    job_triggered = False
    job_execution_id = None
    
    try:
        # gcloudÎ°ú Job Ïã§Ìñâ (ÎπÑÎèôÍ∏∞)
        cmd = [
            "gcloud", "run", "jobs", "execute", job_name,
            f"--region={region}",
            f"--project={project_id}",
            "--async",  # ÎπÑÎèôÍ∏∞ Ïã§Ìñâ
            "--format=json"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            job_triggered = True
            # Job execution ID Ï∂îÏ∂ú ÏãúÎèÑ
            try:
                import json
                output = json.loads(result.stdout)
                job_execution_id = output.get("metadata", {}).get("name", "").split("/")[-1]
            except:
                job_execution_id = "triggered"
            logger.info(f"[SCHEDULE/RUN] Worker Job triggered: {job_execution_id}")
        else:
            logger.error(f"[SCHEDULE/RUN] Job trigger failed: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logger.error("[SCHEDULE/RUN] Job trigger timeout")
    except FileNotFoundError:
        logger.warning("[SCHEDULE/RUN] gcloud not found, falling back to inline execution")
        # gcloud ÏóÜÏúºÎ©¥ Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú Ìè¥Î∞±
        return await _run_generation_inline(today, start_time)
    except Exception as e:
        logger.error(f"[SCHEDULE/RUN] Job trigger error: {e}")

    duration = time.time() - start_time
    
    if job_triggered:
        db_log(
            LogCategory.SCHEDULER,
            f"Worker Job triggered for {today} (execution_id={job_execution_id})",
            LogLevel.INFO,
            "schedule_run"
        )
        
        return ScheduleRunResponse(
            status="job_triggered",
            data_generated=False,  # JobÏù¥ Ï≤òÎ¶¨Ìï† ÏòàÏ†ï
            problems_generated=0,
            duration_sec=round(duration, 2),
            date=str(today),
            details={
                "job_name": job_name,
                "execution_id": job_execution_id,
                "message": "Worker Job triggered asynchronously. Check job logs for progress."
            }
        )
    else:
        return ScheduleRunResponse(
            status="trigger_failed",
            data_generated=False,
            problems_generated=0,
            duration_sec=round(duration, 2),
            date=str(today),
            details={"error": "Failed to trigger Worker Job"}
        )


async def _run_generation_inline(today, start_time):
    """Ìè¥Î∞±: Ïù∏ÎùºÏù∏ ÏÉùÏÑ± (Worker Job ÏóÜÏùÑ Îïå)"""
    import time
    from backend.common.logging import get_logger
    logger = get_logger(__name__)
    
    data_generated = False
    problems_generated = 0
    details = {"date": str(today), "fallback": True}
    
    # 1. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (PAÎßå)
    try:
        from backend.generator.data_generator_advanced import generate_data
        generate_data(modes=("pa",))
        data_generated = True
        logger.info("[SCHEDULE/RUN] Fallback: PA data generation completed")
    except Exception as e:
        logger.error(f"[SCHEDULE/RUN] Fallback: Data generation failed: {e}")
        details["data_error"] = str(e)

    # 2. PA Î¨∏Ï†ú ÏÉùÏÑ±
    try:
        from problems.generator import generate as gen_pa
        with postgres_connection() as pg:
            gen_pa(today, pg)
            
            pa_df = pg.fetch_df("""
                SELECT COUNT(*) as cnt FROM public.problems
                WHERE problem_date = %s AND data_type = 'pa'
            """, [today])
            pa_count = int(pa_df.iloc[0]["cnt"]) if len(pa_df) > 0 else 0
            details["pa_problems"] = pa_count
            problems_generated += pa_count
            
        logger.info(f"[SCHEDULE/RUN] Fallback: PA problems generated: {pa_count}")
    except Exception as e:
        logger.error(f"[SCHEDULE/RUN] Fallback: PA problem generation failed: {e}")
        details["pa_error"] = str(e)

    duration = time.time() - start_time
    status = "success" if problems_generated > 0 else "error"

    db_log(
        LogCategory.SCHEDULER,
        f"Fallback generation completed: {status} (problems={problems_generated})",
        LogLevel.INFO if status == "success" else LogLevel.WARNING,
        "schedule_run"
    )

    return ScheduleRunResponse(
        status=status,
        data_generated=data_generated,
        problems_generated=problems_generated,
        duration_sec=round(duration, 2),
        date=str(today),
        details=details
    )

